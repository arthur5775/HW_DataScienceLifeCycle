{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Laboratory Notes - Week 9: More Model Building and Serialising/De-serialising Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered:\n",
    "\n",
    "* Data Collection\n",
    "* Data Wrangling\n",
    "* Data Preparation & Data Analysis\n",
    "* Data Visualisation\n",
    "\n",
    "What do we do with our machine learning models?  This week's laboratory has a few objectives:\n",
    "\n",
    "* To provide more experience with classifiers\n",
    "* To vary the training dataset sizes, the more training data the better\n",
    "* To vary the number of features, the more the better until a certain amount\n",
    "* To serialise/deserialise your model\n",
    "* To serialise and share it with another analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the titanic dataset that we used before.\n",
    "\n",
    "<span style=\"color:red\">\"\"\"import libraries\"\"\"  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.preprocessing import StandardScaler \"\"\"import here but not used, you can try to use it to get better results\"\"\"  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "%matplotlib inline</span>  \n",
    "\n",
    "<span style=\"color:red\">dataset = pd.read_csv('titanic.csv')</span>\n",
    "\n",
    "You can proceed to do the data auditing to view the dataset\n",
    "\n",
    "<span style=\"color:red\">dataset.shape  \n",
    "dataset.head()</span>\n",
    "\n",
    "<span style=\"color:red\">X = dataset.iloc[:,2:11].values  # The other unprocessed features  \n",
    "y = dataset.iloc[:,1].values # We want the \"Survived\" as the label as we are predicting if the passenger survived the disaster</span>  \n",
    "\n",
    "Let's inspect a row of the features X, and a row of the label y.\n",
    "\n",
    "<span style=\"color:red\">X[0]  \n",
    "y[3]</span>\n",
    "\n",
    "We proceed to split our dataset for training and testing.\n",
    "\n",
    "<span style=\"color:red\">\"\"\"We start with 20% for training dataset\"\"\"  \n",
    "\"\"\"Do vary the random_state\"\"\"  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 0)</span>\n",
    "\n",
    "Then we build our model\n",
    "\n",
    "<span style=\"color:red\">classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)  \n",
    "classifier.fit(X_train, y_train)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss in class and among yourselves why there is an error and the first error stated is\n",
    "\n",
    "ValueError: could not convert string to float: 'Kelly, Mr. James'\n",
    "\n",
    "What does this mean?  There are a few things that we need to take into consideration when we are doing the model building. Let's look at the features available and decide which we should include or which feature would not make a difference in the prediction. The features\n",
    "\n",
    "* Passenger ID\n",
    "* Name\n",
    "* Ticket\n",
    "\n",
    "would not have an impact on whether the person survived or not. On the other hand,\n",
    "\n",
    "* SibSp (siblings and/or spouse)\n",
    "* Parch (parents and/or children)\n",
    "\n",
    "may have an influence on survivability. What about\n",
    "\n",
    "* Fare\n",
    "* Embarked\n",
    "\n",
    "We aren't sure.  Let's remove the 3 that will not impact the prediction. We get the names of the columns for clarity.\n",
    "\n",
    "<span style=\"color:red\">dataset.columns</span>\n",
    "\n",
    "Instead of subsetting using the column index, let's use the column names.\n",
    "\n",
    "<span style=\"color:red\">dataset = dataset.drop(columns=['PassengerId', 'Name', 'Ticket'])</span>\n",
    "\n",
    "Let's attempt to feed this into the decision tree classifier again.\n",
    "\n",
    "<span style=\"color:red\">X = dataset.iloc[:,1:8].values  \"\"\"The other unprocessed features\"\"\"  \n",
    "y = dataset.iloc[:,0].values</span>\n",
    "\n",
    "<span style=\"color:red\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 0)</span>\n",
    "\n",
    "<span style=\"color:red\">classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)  \n",
    "classifier.fit(X_train, y_train)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You still have errors. If you didn't figure out the actual error in Exercise 1, try again.\n",
    "\n",
    "If you have figured out from the errors message earlier, it means that the Decision Tree algorithm (DecisionTreeClassifier) needs the input to be of datatype 'float'. It means that you cannot have categorical data as the input and the input has to be numeric and to be specific, real numbers, or floating point numbers in computing language.\n",
    "\n",
    "We will need to represent these categorical data as unique numbers in the columns. Let's look at the column types of the DataFrame.\n",
    "\n",
    "dataset.dtypes\n",
    "\n",
    "We have 'Sex', 'Cabin' and 'Embarked' as object, which means they are categorical data. How do we represent them? We have some Python functions for this, but let's look at the possible values in the columns.\n",
    "\n",
    "pd.unique(dataset['Sex'])\n",
    "\n",
    "pd.unique(dataset['Cabin'])\n",
    "\n",
    "pd.unique(dataset['Embarked'])\n",
    "\n",
    "For the 'Sex', it seems straightforward enough, with just 2 options. We can simply select to represent 'male' with 0 and 'female' with 1. We will take this opportunity to also introduce the function map() which maps the values. (We have introduced you filter() somewhere earlier).\n",
    "\n",
    "dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "There are many cabin types and there are 3 embarkation ports and an NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we decide to do something with the NaN, or do we remove these?  (Please discuss and/or ask in class)\n",
    "\n",
    "Let's find out how many of the rows have NaN in the 'Embarked' column\n",
    "\n",
    "dataset['Embarked'].isna().sum()\n",
    "\n",
    "There seem to be only 2! Why don't we remove those rows.\n",
    "\n",
    "dataset = dataset.dropna(subset=['Embarked'])\n",
    "dataset.shape\n",
    "\n",
    "dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q':2} ).astype(int)\n",
    "\n",
    "What about 'Cabin'?\n",
    "\n",
    "dataset['Cabin'].isna().sum()\n",
    "\n",
    "That's almost 80% of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we use 'Cabin'?\n",
    "\n",
    "Look at the 1st class cabin identifier and the 2nd class cabin identifier. We can think of 'Cabin' as either the passenger has a cabin or otherwise. However, for this class, let's just drop the column 'Cabin' as it would have a very strong correlation with 1st and 2nd class anyway.  (We are just taking some shortcuts here but we should process and ensure that there is a strong correlation before we remove the column).\n",
    "\n",
    "dataset = dataset.drop(columns=['Cabin'])\n",
    "dataset.shape\n",
    "\n",
    "dataset.head(30)\n",
    "\n",
    "On checking, we have not cleared all the NaN and it looks like that there are many.\n",
    "\n",
    "dataset['Age'].isna().sum()\n",
    "\n",
    "We can use standard deviation to work out a suitable deviation from the mean to impute the values. However, since 177 out of 889 is a reduction that still allows us to have 700+ entries, let's remove those.  (Again, we are taking shortcuts here, we should attempt to figure out if imputing values can build better models).\n",
    "\n",
    "dataset = dataset.dropna(subset=['Age'])\n",
    "dataset.shape\n",
    "\n",
    "Let's try again! Let's hope we are ready! (You can also try this without removing those Age rows that have NaN and look at the new error message).\n",
    "\n",
    "X = dataset.iloc[:,1:7].values  # We now only have 7 features\n",
    "y = dataset.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data - More is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the following for various training and testing dataset splits\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.8, random_state = 0\n",
    ")\n",
    "classifier = DecisionTreeClassifier(\n",
    "    criterion = 'entropy', random_state = 0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "Let's try to test our model that used 20% of 712 rows to train, about 142 rows.\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix # You can include this when you imported all the libraries earlier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "Accuracy = (TP + TN) / Population = 428 / 570 = 75%\n",
    "\n",
    "Go back and repeat the training and testing split to use 80% of the data for training.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 0\n",
    ")\n",
    "classifier = DecisionTreeClassifier(\n",
    "    criterion = 'entropy', random_state = 0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "A smaller total number because the testing dataset is now just 20% of the 712. The accuracy now is 112 / 143 = 78%\n",
    "\n",
    "Although it does not seem like a large improvement, more training data will usually result in a better model. Do note that the model accuracy may plateau around 75% - 78%. You can try to build your best model.\n",
    "\n",
    "Other than the general guideline that more data will result in a better model, in a similar way more features will result in better models. The caveat here is that the number of features is heavily dependent on whether they have influence on the prediction and also, as a rule of thumb commercially, we usually use about 12 - 18 features. Let's illustrate this with fewer features. We will assume that where they embarked from has little impact, and how much they paid also has little impact. So, we choose the last 3 features.\n",
    "\n",
    "X = dataset.iloc[:,4:7].values  # We now only have 3 features\n",
    "y = dataset.iloc[:,0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 0\n",
    ")\n",
    "\n",
    "classifier = DecisionTreeClassifier(\n",
    "    criterion = 'entropy', random_state = 0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your best model!  Hint, you probably can get better results by categorizing the 'Age' into a range (e.g., 0 - 18, 19 - 25, 25 - 35, etc.).  This would mean that it is discretised and hence is categorical data.  This is also a form of normalisation.  The other column that you may want to do something with would be the 'Fare' feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialisation and De-serialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with your best model, let's exchange your model with your classmates WITHOUT disclosing your training parameters!\n",
    "\n",
    "import pickle # It should already be included in your Python installation.\n",
    "filename = 'final_model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb')) # The 'w' is for write, and the 'b' is for binary\n",
    "\n",
    "You can also view it using the dumps() function instead of dump().\n",
    "\n",
    "pickle.dumps(classifier)\n",
    "\n",
    "You now have a file that is serialised called 'final_model.sav'. You can share this with your classmate and ask them to test it.\n",
    "\n",
    "friends_classifier = pickle.load(open('final_model.sav','rb')) # name may differ and make sure you don't load your own again\n",
    "y_pred = friends_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "Note that you can use any other test dataset that you have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your friend's model have better accuracy? Was it the same for him/her as well?\n",
    "\n",
    "In this laboratory, you would have experienced:\n",
    "\n",
    "* Data preparation takes most of the effort.\n",
    "* New map() function.\n",
    "* Learning algorithms need the input to generally be in numeric format.\n",
    "* More data will generally result in better models as the algorithm has more to learn from.\n",
    "* More features will also generally result in better models, but there are limitations to how many features.\n",
    "* Normalisation, in this case categorisation helps.\n",
    "* Serialisation and de-serialisation of model objects to share without disclosing how you built the model, or simply when you want to use it later.\n",
    "\n",
    "Hope you have had fun so far.  We will have one more laboratory session where we will move away from using the Jupyter Notebook and will serve our model as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My code part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Clustering (k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# This is a library we import to run the K-means clustering algorithm as a blackbox\n",
    "# For more information please see: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.cluster import KMeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
