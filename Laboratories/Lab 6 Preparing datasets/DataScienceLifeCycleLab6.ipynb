{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Laboratory Notes - Week 7: Preparing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our basic libraries for data science, we are introducing the scikit-learn library.  This library is built on top of\n",
    "\n",
    "* numpy  \n",
    "* pandas  \n",
    "* matplotlib  \n",
    "\n",
    "that we have already introduced earlier. We will use this library to\n",
    "\n",
    "* pre-process the datasets  \n",
    "* implement machine learning algorithms  \n",
    "* apply evaluation metrics  \n",
    "\n",
    "for the next 3 to 4 weeks. In practice, we use libraries such as scikit-learn as we don’t want to recreate a complex algorithm every time we want to use it. Scikit-learn is a library in Python that provides numerous functions for data pre-processing, unsupervised and supervised learning algorithms, and various evaluation metrics. The functionality that scikit-learn provides include:\n",
    "\n",
    "* Preprocessing, including Min-Max Normalization  \n",
    "* Regression, including Linear and Logistic Regression  \n",
    "* Classification, including K-Nearest Neighbours  \n",
    "* Clustering, including K-Means  \n",
    "* Model selection through evaluation metrics  \n",
    "\n",
    "Let's start by importing scikit-learn.\n",
    "\n",
    "<span style=\"color:red\">import sklearn</span>\n",
    "\n",
    "Let's go into something more specific first.  Let's have a look at normalisation.\n",
    "\n",
    "<span style=\"color:red\">from sklearn import preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a \"seed\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, we need to understand the term \"seed\" in generating numbers.  A seed, like a plant, signifies the initialisation.  In this case it is a number that is used to initialise a pseudo random number generator.  A computer is a state machine.  A statement machine is one which can be in one of a set number of stable conditions depending on its previous condition and on the present values of its inputs.  Hence, technically it cannot generate real random numbers and it generates pseudo random numbers that are based on a specific equation.  The initialisation of this equation is typically machine clock based or some other default start and can be set by fixing the seed.  This means that if we always use the same seed, the equation will generate the same set of pseudo random numbers.\n",
    "\n",
    "This is important for us to be able to reproduce our results consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not go into details here (as it is part of your coursework 2 :-) ) but will give you an idea of what is expected.  Use the normalize() function in scikit-learn, e.g., the  sklearn.preprocessing.normalize().  We will use this to illustrate the normalisation of a vector (array-like) dataset.\n",
    "\n",
    "<span style=\"color:red\">from sklearn import preprocessing</span>\n",
    "\n",
    "The <span style=\"color:red\">normalize()</span> function is used to scale vector items individually to a unit so that the vector has a length of one. By default the function normalize() uses the square root of the sum of squares of each value, also known as the Euclidean norm, or just L2. If we use L1, then it will normalise where the sum of all the values will be 1.0.  Do note that the <span style=\"color:red\">normalize()</span> function results in values between 0 and 1, but it is not the same as simply scaling the values to fall between 0 and 1.  Let's normalise a one dimensional numpy array.\n",
    "\n",
    "<span style=\"color:red\">import numpy as np  \n",
    "x_array = np.random.randint(8, size=10)</span>\n",
    "\n",
    "The above statement creates 8 integers between 0 - 9 (size).  Use the normalize() function on the array to normalize data along a row, in this case a one dimensional array:\n",
    "\n",
    "<span style=\"color:red\">normalised_l2 = preprocessing.normalize([x_array])  \n",
    "print(normalised_l2)</span>\n",
    "\n",
    "Run the the complete example code to demonstrate how to normalise a numpy array using the normalize() function.  Observe the output.  Now, rerun (including the random number generator) it a few times, and observe the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you get the same output?\n",
    "\n",
    "The output should show that all the values are now in the range between 0 and 1.  If you try to manually compute it, square each value in the output and then add them together, you should get 1 as a result (or very close to 1 allowing for some rounding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <span style=\"color:red\">numpy.square()</span> function and the <span style=\"color:red\">numpy.sum()</span> function, check it results in a number close to 1.0.\n",
    "\n",
    "When you rerun the <span style=\"color:red\">np.random.randint()</span> function, you will get a different set of integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you get the same output each time you run it?\n",
    "\n",
    "Hint: https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html\n",
    "\n",
    "In the function <span style=\"color:red\">normalize()</span>, you can use a different method, called \"<span style=\"color:red\">l1</span>\" and it will produce fractions that add up to 1.0.   Execute the following:\n",
    "\n",
    "<span style=\"color:red\">normalised_l1 = preprocessing.normalize([x_array], norm=\"l1\")  \n",
    "print(normalised_l1)</span>\n",
    "\n",
    "You can use the <span style=\"color:red\">numpy.sum() function to add all the elements together.\n",
    "\n",
    "<span style=\"color:red\">np.sum(normalised_l1)</span>\n",
    "\n",
    "You should get 1.0 as the result.  For other normalisations under sklearn, you can explore\n",
    "\n",
    "* sklearn.preprocessing.MinMaxScaler (Min-Max range scaler)  \n",
    "* sklearn.preprocessing.StandardScaler (Z-Score, using standard deviation)  \n",
    "* sklearn.preprocessing.FunctionTransformer (Specify log_transform)\n",
    "\n",
    "You will normally need to understand your data before applying any normalisation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at splitting the data.  We will use the titanic dataset for this exercise.\n",
    "\n",
    "<span style=\"color:red\">import pandas as pd  \n",
    "titanic = pd.read_csv(\"titanic.csv\")  \n",
    "titanic.shape  \n",
    "titanic.describe()</span>\n",
    "\n",
    "The above tells us (reminds us) of the size of the dataset, which should consist of 891 rows (observations) and 12 columns (attributes).  The describe() will give us a statistical summary of the dataset that we have (which includes the 5-number summary).  Do note the statistics for the dataset.  If we are to split the dataset for training and testing, and we do it manually, e.g., we use about 800 observations for training and 91 observations for testing.\n",
    "\n",
    "<span style=\"color:red\">train_data = titanic[0:799]  \n",
    "test_data = titanic[800:]</span>\n",
    "\n",
    "Once we have the split, do a describe() for the training and testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.4: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the statistics (just use the 5-number summary) consistent with the original titanic dataset?  Which column would NOT be of interest?\n",
    "\n",
    "Have a look at the \"Fare\" column.  Now, let's introduce a built-in function in scikit-learn for this.\n",
    "\n",
    "<span style=\"color:red\">from sklearn.model_selection import train_test_split</span>\n",
    "\n",
    "We then ask it to keep 90% for training and 10% for testing by specifying that we want 0.1 of the dataset for testing.\n",
    "\n",
    "<span style=\"color:red\">train, test = train_test_split(titanic, test_size=0.1)</span>\n",
    "\n",
    "Do note the syntax for Python function that returns 2 DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.5: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the train and test datasets more consistent in terms of the 5-number summary for the \"Fare\" column?  What do you think the function did?\n",
    "\n",
    "Side note: The <span style=\"color:red\">train_test_split()</span> function can accept more than 1 DataFrame input and usually it is used to also create appropriate training and testing datasets with the respective labels, e.g., \n",
    "\n",
    "<span style=\"color:red\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)</span>\n",
    "\n",
    "* The \"<span style=\"color:red\">X</span>\" is the features for the training and for the testing, whereas the \"y\" are the corresponding labels.\n",
    "* The <span style=\"color:red\">test_size</span> in this case specifies that 30% of the observations are to be kept for testing.\n",
    "* The <span style=\"color:red\">random_state</span> is the \"seed\" number to ensure that the analyst can reproduce the data split.\n",
    "\n",
    "If we have normalised the data, and then we split the training and testing datasets with the respective labels, we are ready for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss among yourselves, which of the titanic columns (attributes) should be used to build a model to predict whether the titanic passenger survived and which of the column (attribute) may require normalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My code part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60302269 0.30151134 0.30151134 0.         0.10050378 0.40201513\n",
      "  0.20100756 0.40201513 0.20100756 0.20100756]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_array = np.random.randint(8, size=10)\n",
    "\n",
    "normalised_l2 = preprocessing.normalize([x_array])\n",
    "print(normalised_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array: [6 3 3 0 1 4 2 4 2 2]\n",
      "L2 Normalized Array: [[0.60302269 0.30151134 0.30151134 0.         0.10050378 0.40201513\n",
      "  0.20100756 0.40201513 0.20100756 0.20100756]]\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Original Array:\", x_array)\n",
    "print(\"L2 Normalized Array:\", normalised_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since np.random.randint() generates different random numbers each time, the output will change every time you execute it.  \n",
    "The normalization scales the vector so that the sum of the squares of its elements equals 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array: [3 1 3 0 6 0 0 1 5 3]\n",
      "L2 Normalized Array: [[0.31622777 0.10540926 0.31622777 0.         0.63245553 0.\n",
      "  0.         0.10540926 0.52704628 0.31622777]]\n",
      "Sum of Squares: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Generate a random array of integers between 0 and 7\n",
    "x_array = np.random.randint(8, size=10)\n",
    "\n",
    "# Normalize using L2 norm\n",
    "normalised_l2 = preprocessing.normalize([x_array], norm='l2')\n",
    "\n",
    "# Compute the sum of squares\n",
    "sum_of_squares = np.sum(np.square(normalised_l2))\n",
    "\n",
    "# Print results\n",
    "print(\"Original Array:\", x_array)\n",
    "print(\"L2 Normalized Array:\", normalised_l2)\n",
    "print(\"Sum of Squares:\", sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array: [6 3 4 6 2 7 4 4 6 1]\n",
      "L2 Normalized Array: [[0.40544243 0.20272121 0.27029495 0.40544243 0.13514748 0.47301616\n",
      "  0.27029495 0.27029495 0.40544243 0.06757374]]\n",
      "Sum of Squares (L2 Norm): 1.0\n",
      "L1 Normalized Array: [[0.13953488 0.06976744 0.09302326 0.13953488 0.04651163 0.1627907\n",
      "  0.09302326 0.09302326 0.13953488 0.02325581]]\n",
      "Sum of Absolute Values (L1 Norm): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate a random array of integers between 0 and 7\n",
    "x_array = np.random.randint(8, size=10)\n",
    "\n",
    "# L2 Normalization\n",
    "normalised_l2 = preprocessing.normalize([x_array], norm=\"l2\")\n",
    "sum_of_squares = np.sum(np.square(normalised_l2))\n",
    "\n",
    "# L1 Normalization\n",
    "normalised_l1 = preprocessing.normalize([x_array], norm=\"l1\")\n",
    "sum_of_abs = np.sum(normalised_l1)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Array:\", x_array)\n",
    "print(\"L2 Normalized Array:\", normalised_l2)\n",
    "print(\"Sum of Squares (L2 Norm):\", sum_of_squares)  # Should be ~1.0\n",
    "\n",
    "print(\"L1 Normalized Array:\", normalised_l1)\n",
    "print(\"Sum of Absolute Values (L1 Norm):\", sum_of_abs)  # Should be 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array:\n",
      " [6 3 4 6 2 7 4 4 6 1]\n",
      "\n",
      "Min-Max Scaled:\n",
      " [0.83333333 0.33333333 0.5        0.83333333 0.16666667 1.\n",
      " 0.5        0.5        0.83333333 0.        ]\n",
      "\n",
      "Standard Scaled (Z-score):\n",
      " [ 0.92060161 -0.70398947 -0.16245911  0.92060161 -1.24551983  1.46213197\n",
      " -0.16245911 -0.16245911  0.92060161 -1.78705019]\n",
      "\n",
      "Log Transformed:\n",
      " [1.94591015 1.38629436 1.60943791 1.94591015 1.09861229 2.07944154\n",
      " 1.60943791 1.60943791 1.94591015 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate a random array of integers between 0 and 7\n",
    "x_array = np.random.randint(8, size=(10, 1))  # Reshape needed for scalers\n",
    "\n",
    "# Min-Max Scaling (scales values between 0 and 1)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "x_minmax = minmax_scaler.fit_transform(x_array)\n",
    "\n",
    "# Standard Scaling (Z-score: mean = 0, std = 1)\n",
    "standard_scaler = StandardScaler()\n",
    "x_standard = standard_scaler.fit_transform(x_array)\n",
    "\n",
    "# Log Transformation (apply natural logarithm)\n",
    "log_transformer = FunctionTransformer(np.log1p)  # log1p(x) = log(x + 1) to avoid log(0)\n",
    "x_log = log_transformer.transform(x_array)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Array:\\n\", x_array.flatten())\n",
    "print(\"\\nMin-Max Scaled:\\n\", x_minmax.flatten())\n",
    "print(\"\\nStandard Scaled (Z-score):\\n\", x_standard.flatten())\n",
    "print(\"\\nLog Transformed:\\n\", x_log.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PassengerId",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Survived",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pclass",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SibSp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Parch",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fare",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "21a2e2af-1b3d-47dd-a492-6ce61b8ec738",
       "rows": [
        [
         "count",
         "891.0",
         "891.0",
         "891.0",
         "714.0",
         "891.0",
         "891.0",
         "891.0"
        ],
        [
         "mean",
         "446.0",
         "0.3838383838383838",
         "2.308641975308642",
         "29.69911764705882",
         "0.5230078563411896",
         "0.38159371492704824",
         "32.204207968574636"
        ],
        [
         "std",
         "257.3538420152301",
         "0.4865924542648575",
         "0.836071240977049",
         "14.526497332334042",
         "1.1027434322934317",
         "0.8060572211299483",
         "49.6934285971809"
        ],
        [
         "min",
         "1.0",
         "0.0",
         "1.0",
         "0.42",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "223.5",
         "0.0",
         "2.0",
         "20.125",
         "0.0",
         "0.0",
         "7.9104"
        ],
        [
         "50%",
         "446.0",
         "0.0",
         "3.0",
         "28.0",
         "0.0",
         "0.0",
         "14.4542"
        ],
        [
         "75%",
         "668.5",
         "1.0",
         "3.0",
         "38.0",
         "1.0",
         "0.0",
         "31.0"
        ],
        [
         "max",
         "891.0",
         "1.0",
         "3.0",
         "80.0",
         "8.0",
         "6.0",
         "512.3292"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"data/titanic.csv\")\n",
    "titanic.shape\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = titanic[0:799]\n",
    "test_data = titanic[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(titanic, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "        PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Training dataset:\n",
      "        PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   801.000000  801.000000  801.000000  640.000000  801.000000   \n",
      "mean    445.363296    0.374532    2.334582   29.145703    0.531835   \n",
      "std     258.484340    0.484304    0.827898   14.122288    1.106480   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     221.000000    0.000000    2.000000   20.000000    0.000000   \n",
      "50%     444.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.000000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   71.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  801.000000  801.000000  \n",
      "mean     0.382022   31.352309  \n",
      "std      0.810171   47.802900  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.895800  \n",
      "50%      0.000000   14.108300  \n",
      "75%      0.000000   30.070800  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Testing dataset:\n",
      "        PassengerId   Survived     Pclass        Age      SibSp      Parch  \\\n",
      "count    90.000000  90.000000  90.000000  74.000000  90.000000  90.000000   \n",
      "mean    451.666667   0.466667   2.077778  34.485405   0.444444   0.377778   \n",
      "std     248.389238   0.501683   0.877020  17.010815   1.071711   0.772822   \n",
      "min      24.000000   0.000000   1.000000   0.920000   0.000000   0.000000   \n",
      "25%     253.250000   0.000000   1.000000  24.000000   0.000000   0.000000   \n",
      "50%     457.500000   0.000000   2.000000  31.500000   0.000000   0.000000   \n",
      "75%     669.250000   1.000000   3.000000  43.000000   1.000000   0.000000   \n",
      "max     880.000000   1.000000   3.000000  80.000000   8.000000   3.000000   \n",
      "\n",
      "             Fare  \n",
      "count   90.000000  \n",
      "mean    39.786111  \n",
      "std     63.947837  \n",
      "min      0.000000  \n",
      "25%      8.662500  \n",
      "50%     26.000000  \n",
      "75%     51.272925  \n",
      "max    512.329200  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original dataset:\\n\", titanic.describe())\n",
    "print(\"\\nTraining dataset:\\n\", train.describe())\n",
    "print(\"\\nTesting dataset:\\n\", test.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Fare summary:\n",
      " count    891.000000\n",
      "mean      32.204208\n",
      "std       49.693429\n",
      "min        0.000000\n",
      "25%        7.910400\n",
      "50%       14.454200\n",
      "75%       31.000000\n",
      "max      512.329200\n",
      "Name: Fare, dtype: float64\n",
      "\n",
      "Training Fare summary:\n",
      " count    801.000000\n",
      "mean      31.352309\n",
      "std       47.802900\n",
      "min        0.000000\n",
      "25%        7.895800\n",
      "50%       14.108300\n",
      "75%       30.070800\n",
      "max      512.329200\n",
      "Name: Fare, dtype: float64\n",
      "\n",
      "Testing Fare summary:\n",
      " count     90.000000\n",
      "mean      39.786111\n",
      "std       63.947837\n",
      "min        0.000000\n",
      "25%        8.662500\n",
      "50%       26.000000\n",
      "75%       51.272925\n",
      "max      512.329200\n",
      "Name: Fare, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Fare summary:\\n\", titanic[\"Fare\"].describe())\n",
    "print(\"\\nTraining Fare summary:\\n\", train[\"Fare\"].describe())\n",
    "print(\"\\nTesting Fare summary:\\n\", test[\"Fare\"].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Selecting Relevant Features (\"X\")  \n",
    "A. Potentially Useful Features  \n",
    "Key variables (strongly correlated with survival):  \n",
    "\n",
    "* Pclass (Passenger class) → First-class passengers had a higher survival rate.  \n",
    "* Sex (Gender) → Women had a higher chance of survival.  \n",
    "* Age → Children were prioritized for lifeboats.  \n",
    "* SibSp (Number of siblings/spouses aboard) → May influence survival chances.  \n",
    "* Parch (Number of parents/children aboard) → Family members may have helped each other.  \n",
    "* Fare (Ticket price) → Could be correlated with Pclass (higher fare = first-class).  \n",
    "* Embarked (Port of embarkation) → Might indicate socioeconomic status and class.  \n",
    "\n",
    "B. Irrelevant or Less Useful Features  \n",
    "Features to exclude or with limited predictive power:  \n",
    "\n",
    "PassengerId → Just an identifier, no predictive value.  \n",
    "* Name: Unique to each passenger, difficult to use meaningfully.  \n",
    "* Ticket: No clear impact on survival.  \n",
    "* Cabin: Too many missing values, challenging to use without advanced imputation.  \n",
    "\n",
    "2) Which Features Need Normalization?  \n",
    "Normalization is generally applied to continuous numerical variables to prevent certain features from dominating others when training the model.  \n",
    "\n",
    "Features that need normalization (e.g., MinMaxScaler or StandardScaler):  \n",
    "* Age: Ranges from 0 to 80, needs scaling.  \n",
    "* Fare: Can range from 0 to over 500, has extreme values.  \n",
    "\n",
    "Features that do not necessarily require normalization:  \n",
    "* Pclass, SibSp, Parch → These are discrete values, so normalization is not necessary but possible.  \n",
    "\n",
    "Categorical features that need encoding:  \n",
    "\n",
    "* Sex (male, female) → Convert to 0 and 1.  \n",
    "* Embarked (C, Q, S) → Use One-Hot Encoding to avoid introducing an arbitrary order.  \n",
    "\n",
    "3) Conclusion: Data Preprocessing Pipeline  \n",
    "Recommended preprocessing before training the model:  \n",
    "\n",
    "* Encode categorical variables (Sex, Embarked).  \n",
    "* Impute missing values (Age, Embarked, Fare).  \n",
    "* Normalize continuous variables (Age, Fare).  \n",
    "* Select final features (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
