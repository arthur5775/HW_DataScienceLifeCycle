{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Laboratory Notes - Week 8: Clustering and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we approach the end parts of our standard life cycle, we will relook at most of what we have covered in lectures and previous tutorials through experiencing some clustering with k-means and classification with decision trees.  The laboratory for this week will include what you have already done, e.g.,\n",
    "\n",
    "* Data Auditing  \n",
    "* Normalisation  \n",
    "* Training and Testing Dataset Split  \n",
    "* Visualisation  \n",
    "\n",
    "At the end, you can also explore some tools, especially cloud machine learning services (which change very frequently, so you will need to keep updated on them regularly - and also the links provided may be outdated - last checked 7 March 2025).\n",
    "\n",
    "Through Week 9 Laboratory exercises, in addition to experimenting with k-means, and Decision Trees (DT), we will also focus a little on understanding the measurement metrics and do share with your peers on how you interpret these metrics for a stakeholder's understanding and more importantly for a stakeholder's appreciation of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start Week 9 with clustering. This is an area of unsupervised machine learning, where there is no training and testing of the models but models are built by finding patterns within the dataset.  We will use the k-means clustering algorithm for this activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Clustering (k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will illustrate a clustering task using k-means clustering in a 2-dimensional space. We will not look at the theory of k-means clustering; we will just use it as a black box in order to understand some fundamental concepts of segmentation and clustering. So this is a tutorial without mathematics (I know I repeat this a lot, but just wanted to ensure that we are focused on the Life Cycle and not the mathematics behind the algorithms). Now it's important to realise that in data science you will rarely use simple 2-dimensional modelling. However, the simple nature of the material means we can carefully study the different aspects of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in k-means clustering is to find k subgroups in a given dataset. Please note the variable k represents the number of subgroups in the data. k-means assigns each data point in the dataset to one of the k subgroups iteratively based on the variables (features) in the dataset.  The pseudo-code (algorithm) of k-means works as:\n",
    "\n",
    "Choose the number of clusters (k)\n",
    "Randomly choose the initial centroids c1, c2, ..... ck\n",
    "For each data point xi:\n",
    "find the nearest centroid (c1, c2, ..... ck) and assign the data point to that cluster\n",
    "For each cluster j = 1..k\n",
    "Move/discover a new centroid where the centroid is the mean of all data points assigned to that cluster\n",
    "Repeat steps 3 and 4 until convergence or until the end of a fixed number of iterations\n",
    "The output of the k-means clustering are:\n",
    "\n",
    "k cluster centroids. The centroids can be used to find the relevant subgroup for a new data point.\n",
    "Cluster assignment (also called labels in this case but not to be confused with labels for supervised machine learning) for all data points in the given dataset.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\" This is a library we import to run the K-means clustering algorithm as a blackbox\"\"\"\n",
    "\"\"\" For more information please see: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Load the dataset from Canvas \n",
    "\n",
    "\"\"\" We use the standard Iris dataset that is very popular for learning clustering.\"\"\"\n",
    "iris_data = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "As usual, we will look at the data and also the size of the dataset, e.g, check the dataset dimension\n",
    "\n",
    "iris_data.shape\n",
    "\n",
    "Let's have a look at a single sample data (instead of head() or tail().\n",
    "\n",
    "iris_data.sample()\n",
    "\n",
    "Column names\n",
    "\n",
    "iris_data.columns\n",
    "\n",
    "How about if we look at the description, since most of the data are numeric (except probably the label). Do note that this dataset has labels but we aren't going to use it for classification, which you can as an extra exercise.\n",
    "\n",
    "iris_data.describe()\n",
    "\n",
    "Visualise\n",
    "Let’s have a visual look at our data points.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x=iris_data['sepalwidth'],y=iris_data['sepallength'])\n",
    "plt.xlabel('Sepal Width (cm)')\n",
    "plt.ylabel('Sepal Length (cm)')\n",
    "\n",
    "![P1](picture/P1.png)\n",
    "\n",
    "From a manual inspection of the data points, we probably can see 2 or 3 groups.  We proceed to ask the machine to learn from the input data to determine the groupings. First thing to note is that for k-means, the machine learning algorithm does not know how many clusters it should determine and the default k-means algorithm expects the user to provide that information.\n",
    "\n",
    "# Run the K-means clustering over the dataset using only\n",
    "# distance and speeding features. Set K=2: we only want\n",
    "# to cluster the dataset into two subgroups\n",
    "kmeans = KMeans(n_clusters=2).fit(\n",
    "    iris_data[['sepalwidth','sepallength']]\n",
    ")\n",
    "\n",
    "Similarly to how we do data auditing, prior to visualisation, we may want to have a look at some of the results of the clustering.\n",
    "\n",
    "Look at the outputs: Two cluster centers (this is known as the cluster centroids)\n",
    "\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "Look at the outputs: Cluster labels.  With k = 2, we will have the labels 0 and 1 to indicate which cluster the data points belong to.\n",
    "\n",
    "kmeans.labels_\n",
    "\n",
    "Visualise k-means output\n",
    "Visualise the output labels\n",
    "\n",
    "plt.scatter(\n",
    "    x=iris_data['sepalwidth'],\n",
    "    y=iris_data['sepallength'], \n",
    "    c=kmeans.labels_)\n",
    "\n",
    "Visualise the cluster centers (black stars)\n",
    "\n",
    "plt.plot(\n",
    "    kmeans.cluster_centers_[:,0],\n",
    "    kmeans.cluster_centers_[:,1],\n",
    "    'k*',\n",
    "    markersize=20\n",
    ")\n",
    "plt.xlabel('SepalWidth')\n",
    "plt.ylabel('SepalLength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look similar to what we envisaged for 2 clusters?\n",
    "\n",
    "Let’s try to run k-means with different k values to get more clusters.\n",
    "\n",
    "\"\"\" Run K-means with another K value\"\"\"\n",
    "\"\"\" Set K=4: we want to cluster the dataset into four subgroups\"\"\"\n",
    "kmeans2 = KMeans(n_clusters=4).fit(\n",
    "    iris_data[['sepalwidth','sepallength']]\n",
    ")\n",
    "\n",
    "Visualise the output labels\n",
    "\n",
    "plt.scatter(\n",
    "    x=iris_data['sepalwidth'],\n",
    "    y=iris_data['sepallength'], \n",
    "    c=kmeans2.labels_\n",
    ")\n",
    "\n",
    "Visualise the cluster centers (black stars)\n",
    "\n",
    "plt.plot(\n",
    "    kmeans2.cluster_centers_[:,0],\n",
    "    kmeans2.cluster_centers_[:,1],\n",
    "    'k*',\n",
    "    markersize=20\n",
    ")\n",
    "\n",
    "plt.xlabel('SepalWidth')\n",
    "plt.ylabel('SepalLength')\n",
    "plt.show()\n",
    "\n",
    "k-means starts with a random positioning for the cluster centres (centroids) and then it computes the distance between the points on the centroid.  As mentioned earlier in our classes, the randomness is pseudo random and we can re-produce it each time by setting a seed.  In this case, let's try to add the parameter init='random' for the k-means initialisation.\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=4, init='random').fit(\n",
    "    iris_data[['sepalwidth','sepallength']]\n",
    ")\n",
    "\n",
    "and run it multiple times.\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=4, init='random').fit(\n",
    "    iris_data[['sepalwidth','sepallength']]\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x=iris_data['sepalwidth'],\n",
    "    y=iris_data['sepallength'], \n",
    "    c=kmeans2.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the clustering change? Why?\n",
    "\n",
    "Visualise the cluster centers (black stars)\n",
    "\n",
    "plt.plot(\n",
    "    kmeans2.cluster_centers_[:,0],\n",
    "    kmeans2.cluster_centers_[:,1],\n",
    "    'k*',\n",
    "    markersize=20\n",
    ")\n",
    "\n",
    "plt.xlabel('SepalWidth')\n",
    "plt.ylabel('SepalLength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our cluster?  There are actually 2 questions in this but we look at something termed as inertia.  In clustering for k-means, the inertia measures how well a dataset was clustered. It is the measurement of the the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster.\n",
    "\n",
    "kmeans.inertia_\n",
    "\n",
    "A good model is one with low inertia and a low number of clusters.  It is a combination of both for a good measure.  The second question is actually what's the best number for k?  There are 2 common ways to determine the best k.  Do note that for larger k, the inertia will typically be lower due to the distance between points and centroid will be smaller, hence it is low inertia AND low k.  The two common ways to determine the number of clusters are (1) Elbow (sometimes known as Knee) method, and (2) Silhouette Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Analysis (Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the error function, which from the inertia, we will use the sum of the squared distances.\n",
    "\n",
    "sum_of_squared_distances = []\n",
    "k = range(2,9)\n",
    "\n",
    "Then we plot for each cluster size chosen (k value), the error function (or inertia).\n",
    "\n",
    "for nclusters in k:\n",
    "    kmeans = KMeans(n_clusters=nclusters).fit(\n",
    "        iris_data[['sepalwidth','sepallength']]\n",
    "    )\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "We then visually (manually) identify the elbow (or knee) point.  This is the point where the gradient plateaus significantly.  You can actually measure the tangent of each and decide but a visual look is sufficient.\n",
    "\n",
    "plt.plot(k, sum_of_squared_distances, 'b-')\n",
    "plt.xlabel('Values of k')\n",
    "plt.ylabel('Sum of Squared Distance')\n",
    "plt.title('Elbow Analysis for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "![P2](picture/P2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more \"scientific\" method would be to use the silhouette score.  (Caveat: From my personal point of view, this is not a good measure but I am including it here as it is a method used relatively widely).  We first import the function from the library.\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "Again, we initialise our coding.\n",
    "\n",
    "silhouette_avg = []\n",
    "k = range(2,9)\n",
    "\n",
    "Similarly to the above, we want to visually look at the silhouette score (but we really don't need do, as we can just get the max() value).\n",
    "\n",
    "for nclusters in k:\n",
    "    kmeans = KMeans(n_clusters=nclusters).fit(\n",
    "        iris_data[['sepalwidth','sepallength']]\n",
    "    )\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette_avg.append(silhouette_score(iris_data[['sepalwidth','sepallength']], cluster_labels))\n",
    "\n",
    "Visuals\n",
    "\n",
    "plt.plot(k, silhouette_avg, 'b-')\n",
    "plt.xlabel('Values of k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score Analysis for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "The higher the silhouette score, the better it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above, what's the best number of clusters?  Can you plot and have a visual look whether the clusters make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next exercise will be on classification, which is a form of supervised machine learning. Similarly to Linear Regression, we will have a training and a testing dataset (as an aside, do look at what validation datasets are and you can discuss this on the Canvas Discussions).  For this activity, we will be attempting to predict whether a person has travelled abroad based on the person’s age and income.  The dataset is on Canvas and it is called \"TravelInfo.csv\".  Let’s start with the usual importing of the necessary libraries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = pd.read_csv('TravelInfo.csv')\n",
    "\n",
    "Investigate the content of the dataset (as we would normally), i.e. what are known as the Data Auditing steps.\n",
    "\n",
    "dataset.shape\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "The dataset should have 400 rows and 3 columns.  The columns should be ‘Age’, ‘Income’ and ‘TravelAbroad’.  This is a record of past incidences or survey data.  As the intention is to build a model, we are going to use this as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the whole dataset as the training data, how do we know how good our model is?\n",
    "\n",
    "We proceed to split the dataset into input data and their corresponding labelled data.  The first two columns are the input data, i.e. the ‘Age’ and the ‘Income’ of which we would like to predict if the person has ‘TravelAbroad’ before.  From the past survey data collected, we have some samples and this ‘TravelAbroad’ is the respective labelled data.  Let’s split the data into X, the input data and y, the labelled data.\n",
    "\n",
    "X = dataset.iloc[:, [0, 1]].values    # Input Data: Age and Income\n",
    "y = dataset.iloc[:, 2].values         # Labeled Data: Travelled or not\n",
    "\n",
    "You can review what X and y consist of.  Since, we need to reserve a portion of the dataset for testing data, we can either simply take a portion of the data or a better way is take from a “random” sample of the data, assuming that we want to keep 75% for the training data and 25% for the testing data.  We can use the function train_test_split() from sklearn.model_selection. You should already be familiar with this from the Linear Regression exercise from the previous week.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.25, random_state = 0\n",
    ")\n",
    "\n",
    "Optionally, you can also do the split manually, taking the first 300 rows as the training dataset and the last 100 rows as the testing dataset.  It is very important to note that the training and testing datasets should not have any overlaps.  We will next look at Normalisation, in this case also known as Feature Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation or Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the range of values of raw data varies widely, in some machine learning algorithms, objective (the learning algorithm) functions will not work properly without normalisation (a.k.a. feature scaling). For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed (heavily influenced) by this particular feature. Therefore, the range of all features should be normalised in order for each feature to contribute proportionately to the final distance. In addition, our motivation here is to visualise the feature space later on.\n",
    "\n",
    "Again, we use a built in function StandardScaler() from sklearn.preprocessing to do this. Note that we apply the fit_transform() to the training dataset and not the test dataset.  The reason is that we want to scale it to have a mean of 0 and a certain standard deviation (this is also known as the Z-Score Normalisation - Do note that in Data Science, some of the terms are equivalent and if you know what they do/mean, you will be ok).  The mean and standard deviation values will be stored in the StandardScaler() and applied to the testing dataset using the transform() (later in the code).\n",
    "\n",
    "\"\"\" Feature Scaling\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test using Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data is ready to be used to build the model.  Note that, in reality, we will usually need to read and wrangle the data before we even start to normalise or standardize the data (although this stage does not necessarily occur in all cases).  Let’s use the Decision Tree algorithm, from sklearn.tree’s DecisionTreeClassifier() function.\n",
    "\n",
    "\"\"\" Fitting Decision Tree Classification to the Training set\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(\n",
    "    criterion = 'entropy', random_state = 0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "Normally, when we want to build a model, most functions will have a method called fit() (note that this is not standard but many functions use this).  At the end of running that code, we now have a model using the Decision Tree algorithm and the model is called classifier (you can call your model whatever, it does not have to be classifier). Since we now have a model built, let’s test how good our model is.  We firstly use the built model with the testing input data (not labels as that’s the output that we want to check against what we already know).  We use the method predict() in the model.  Note that this method’s name is also quite common with other built-in functions for models, but many also do call this pred().\n",
    "\n",
    "\"\"\" Predicting the Test set results\"\"\"\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Measurement using Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of labels, y_pred, that is the output from the prediction of using our testing data.  We now need to compare this y_pred with the actual y_test (the true values) and determine the accuracy of our model’s prediction.  To do so, we want to view it using a confusion matrix.  This is an important concept for classifier comparison.\n",
    "\n",
    "\"\"\" Making the Confusion Matrix\"\"\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "Your confusion matrix will be shown as a 2 by 2 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix output, discuss what are the values of True Negatives, True Positives, False Negatives, and False Positives.  Other than accuracy, would precision or recall be a better measure for this particular case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise (not assessed materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take this opportunity to introduce a new visualisation, called the meshgrid.  For this demonstration, we will look at the test results in the feature space.  You do not need to know the syntax or how it works.  This is just for illustration.\n",
    "\n",
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(\n",
    "    np.arange(\n",
    "        start = X_set[:, 0].min() - 1, \n",
    "        stop = X_set[:, 0].max() + 1, \n",
    "        step = 0.01\n",
    "    ),\n",
    "    np.arange(\n",
    "        start = X_set[:, 1].min() - 1,\n",
    "        stop = X_set[:, 1].max() + 1,\n",
    "        step = 0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.contourf(\n",
    "    X1,\n",
    "    X2,\n",
    "    classifier.predict(\n",
    "        np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "    alpha = 0.75,\n",
    "    cmap = ListedColormap(('red', 'green'))\n",
    ")\n",
    "\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(\n",
    "        X_set[y_set == j, 0], \n",
    "        X_set[y_set == j, 1],\n",
    "        c = ListedColormap(('red', 'green'))(i),\n",
    "        label = j\n",
    "    )\n",
    "    \n",
    "plt.title('Decision Tree Classification (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Income')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test using Random Forest Algorithm (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another classifier is the random forests algorithm, which in essence consists of multiple trees, each based on a random sample of the training data.  They usually perform better than Decision Trees, and such a model is also known as an ensemble method (aggregation from many Decision Trees in this case).  Let’s now fit a Random Forest Classification to the Training set.  The concepts are similar to the above, just that we call a different model.  All the reading of data, splitting of data and data normalization has been done above.\n",
    "\n",
    "\"\"\" Fitting Random Forest Classification to the Training set\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators = 20,\n",
    "    criterion = 'entropy',\n",
    "    random_state = 0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\" Predicting the Test set results\"\"\"\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\"\"\" Making the Confusion Matrix\"\"\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results of the above confusion matrix. What is the prediction accuracy? Compare with the decision tree results.\n",
    "\n",
    "In machine learning algorithms, one way to improve the test accuracy is to tune the parameters (e.g. number of trees in the random forest algorithm).  Tuning based on the parameters of the model building is called hyper-parameter tuning.  Try the above using n_estimators = 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you get better results? \n",
    "\n",
    "This comes to the end of our data analytics process but we will continue to look at the deployment of the models next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Cloud Machine Learning Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now had some exposure to the basics of machine learning, with some understanding of associative rule mining, linear regression, clustering with k-means, and classifiers such as decision trees and random forest.  These provide you with the basic machine learning foundations.\n",
    "\n",
    "Other than the tools that are available in Python, many of the cloud platform providers have their own machine learning algorithms available.  Note that like these Python libraries, the algorithms are standard implementations.  As a data scientist, you should have the necessary background to augment these algorithms to maybe suit your needs better.\n",
    "\n",
    "For the rest of Week 9, you can explore:\n",
    "\n",
    "* [AWS Machine Learning](https://aws.amazon.com/fr/free/machine-learning/)  \n",
    "* [Google Cloud AI and Machine Learning](https://cloud.google.com/products/ai)  \n",
    "* [Microsoft Azure Machine Learning](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account?icid=machine-learning)  \n",
    "* [OpenAI API](https://openai.com/api/)  \n",
    "* [AliCloud Machine Learning](https://www.alibabacloud.com/fr/product/machine-learning?_p_lc=1)  \n",
    "* [IBM Watson Machine Learning](https://www.ibm.com/products/watsonx-ai)  \n",
    "* [Huawei ModelArts AI](https://www.huaweicloud.com/intl/en-us/product/modelarts.html)  \n",
    "\n",
    "You should be able to obtain some free trial credits for some of them, either as a student or as a start-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My code part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
